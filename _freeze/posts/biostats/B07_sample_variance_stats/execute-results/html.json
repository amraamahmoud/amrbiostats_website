{
  "hash": "3be0dad06e8ba39d2617b7fd9c5e78eb",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Demystifying sample variance formula\"\nimage: \"/images/biostats.jpg\"\ndate: \"2024-11-10\"\nformat: \n  html: \n    fig-align: center\n    page-layout: article\n    toc: true\n    toc-depth: 5\n    toc-expand: true\n    lightbox: false\ncrossref: \n  custom: \n    - kind: float\n      reference-prefix: \"Example M.10.\"\n      key: tabset\n      space-before-numbering: false\neditor_options: \n  chunk_output_type: console\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n## Background\n\n-   In the [previous chapter](/posts/biostats/B06_dispersion_stats.qmd), we discussed the measures of variability.\n\n-   Specifically, the variance was discussed as a measure of variability that quantifies how close the observations are to the mean.\n\n-   Intuitively, this is achieved by calculating the average of deviations $(D$) of each observation from the mean:\n\n$$\nD = \\frac{1}{n} \\displaystyle \\sum_{i=1}^{n} (x_i - \\bar{x})\n$$\n\n-   However, the sample variance $(s^2)$ is defined as the average of the squared differences (i.e., deviations) between each observation and the sample mean $(\\bar{x})$:\n\n$$\ns^2 = \\frac{1}{n-1} \\displaystyle \\sum_{i=1}^{n} (x_i - \\bar{x})^2\n$$\n\n-   The above formula has two distinct differences from the previous intuitive definition, namely:\n\n    <i class=\"bi bi-1-circle-fill\" style=\"color: #0171c3; font-size: 22px;\"></i> **Using squared deviations instead of the deviations themselves**:\n\n    -   In a [previous chapter](/posts/biostats/B03_cent_tendency_stats.qmd#sec-mean), we described that the sum of deviations of the observations from the arithmetic mean is always zero because the negative deviations cancel out the positive deviations. This cuses the variance to be zero.\n\n    -   Therefore, squaring the deviations overcomes this issue by making all deviations positive.\n\n::: {.callout-note style=\"color: #0466c8;\"}\n-   Cancelling out of positive and negative deviations can also be achieved by taking the absolute value of the deviations resulting in a measure called the mean absolute deviation from the mean.\n\n-   However, squaring the deviations is preferred for many reasons, for instance:\n\n    -   The squared function is differentiable everywhere, while the absolute value function is not differentiable at zero. Differentiability is important for many statistical and optimization algorithms.\n\n    -   The squared deviations give more weight to larger deviations.\n\n    -   The squaring used in the variance is closely linked to the normal distribution function (that also involves a squared deviation term), which is a cornerstone in most statistical methods.\n\n    -   The squared deviations of observations of any given value is minimized when the mean of the observations is used as the reference point, while the mean absolute deviation is minimized when the median is used as the reference point.\n:::\n\n<i class=\"bi bi-2-circle-fill\" style=\"color: #0171c3; font-size: 22px;\"></i> **The denominator is** $(n-1)$ instead of $(n)$:\n\n-   This is known as Bessel's correction (obtained by multiplying the uncorrected sample variance $s^2_n = \\displaystyle \\frac{1}{n} \\displaystyle \\sum_{i=1}^{n} (x_i - \\bar{x})^2$ by $\\displaystyle \\frac{n}{n-1}$) and is used to correct the bias in the estimation of the population variance from the sample variance.\n\n-   When using a sample to estimate the population variance, the population mean is unknown, in most of the cases, so the sample mean $(\\bar{x})$ is used to estimate the population mean $(\\mu)$.\n\n-   Using $\\bar{x}$ as an estimate of $\\mu$ results in underestimation of the population variance, **particularly when the sample size is small**. This happens because the sum of squares of the deviations of the observations from the sample mean will always be smaller than the sum of squares of the deviations of the observations from the population mean (except when $\\bar{x} = \\mu$).\n\n-   Therefore, using $(n-1)$ in the denominator corrects this underestimation and makes the sample variance an unbiased estimator of the population variance.\n\n-   Let's illustrate this concept with a simple simulation:\n\n    -   Assume that we have a population of $1000$ observations:\n\n\n\n\n        ::: {.cell}\n        \n        ```{.r .cell-code}\n        set.seed(123)\n        population <- \n            rnorm(\n               1000, \n               mean = 50, \n               sd = 10\n            )\n        hist(\n           population, \n           main = \"Population Distribution\", \n           xlab = \"Values\", \n           ylab = \"Density\"\n        )\n        ```\n        \n        ::: {.cell-output-display}\n        ![](B07_sample_variance_stats_files/figure-html/unnamed-chunk-2-1.png){width=672}\n        :::\n        :::\n\n\n\n\n    -   The population mean and variance are:\n\n\n\n\n        ::: {.cell}\n        \n        ```{.r .cell-code}\n        paste(\n           \"Population mean: \", \n           round(mean(population), 2)\n        )\n        ```\n        \n        ::: {.cell-output .cell-output-stdout}\n        \n        ```\n        [1] \"Population mean:  50.16\"\n        ```\n        \n        \n        :::\n        \n        ```{.r .cell-code}\n        var.population <- \n            sum((population - mean(population))^2) / length(population)\n        \n        paste(\n           \"Population variance: \", \n           round(var.population, 2)\n        )\n        ```\n        \n        ::: {.cell-output .cell-output-stdout}\n        \n        ```\n        [1] \"Population variance:  98.25\"\n        ```\n        \n        \n        :::\n        :::\n\n\n\n\n        ::: {.callout-note style=\"color: #0466c8;\"}\n        -   In this particular example, the population mean and variance are known because this is a simulation and we generated the data.\n\n        -   In practice, the population mean and variance are **unknown** and we use the sample mean and variance to estimate them.\n        :::\n\n    -   Now, let's draw a sample of $10$ observations from this population and calculate the uncorrected sample variance:\n\n\n\n\n        ::: {.cell}\n        \n        ```{.r .cell-code}\n        sample <- sample(population, 10)\n        \n        paste(\n           \"Sample mean: \", \n           round(mean(sample), 2)\n        )\n        ```\n        \n        ::: {.cell-output .cell-output-stdout}\n        \n        ```\n        [1] \"Sample mean:  50.45\"\n        ```\n        \n        \n        :::\n        \n        ```{.r .cell-code}\n        var.sample <- \n            sum((sample - mean(sample))^2) / length(sample)\n        \n        paste(\n           \"Uncorrected sample variance: \", \n           round(var.sample, 2)\n        )\n        ```\n        \n        ::: {.cell-output .cell-output-stdout}\n        \n        ```\n        [1] \"Uncorrected sample variance:  89.63\"\n        ```\n        \n        \n        :::\n        :::\n\n\n\n\n    -   As you can see that the $\\bar{x} = 50.45$ and is used as an unbiased estimator of the population mean $\\mu = 50.16$, however, the uncorrected sample variance $s_n^2 = 89.63$ underestimates the population variance $\\sigma^2 = 98.25$.\n\n    -   Let's apply Bessel's correction to the sample variance:\n\n\n\n\n        ::: {.cell}\n        \n        ```{.r .cell-code}\n        var.sample.corrected <- \n            sum((sample - mean(sample))^2) / (length(sample) - 1)\n        \n        # The same result can be also obtained using var(sample)\n        \n        paste(\n           \"Corrected sample variance: \", \n           round(var.sample.corrected, 2)\n        )\n        ```\n        \n        ::: {.cell-output .cell-output-stdout}\n        \n        ```\n        [1] \"Corrected sample variance:  99.59\"\n        ```\n        \n        \n        :::\n        :::\n\n\n\n\n    -   The corrected sample variance is $s^2 = 99.59$ is closer to the population variance than the uncorrected sample variance.\n\n    -   On repeated sampling (i.e., taking many samples with the same size and calculating the sample variance for each sample), the average of the corrected sample variances will be closer to the population variance than the average of the uncorrected sample variances (i.e., the corrected sample variance is an unbiased estimator of the population variance):\n\n\n\n\n        ::: {.cell}\n        \n        ```{.r .cell-code}\n        set.seed(123)\n        n <- 1000\n        sample.var.uncor <- numeric(n)\n        \n        for (i in 1:n) {\n            sample <- sample(population, 10)\n            sample.var.uncor[i] <- \n        sum((sample - mean(sample))^2) / (length(sample))\n        }\n        \n        paste(\n           \"Mean of 1000 uncorrected sample variances: \", \n           round(mean(sample.var.uncor), 2)\n        )\n        ```\n        \n        ::: {.cell-output .cell-output-stdout}\n        \n        ```\n        [1] \"Mean of 1000 uncorrected sample variances:  88.79\"\n        ```\n        \n        \n        :::\n        \n        ```{.r .cell-code}\n        sample.var.cor <- numeric(n)\n        \n        for (i in 1:n) {\n            sample <- sample(population, 10)\n            sample.var.cor[i] <- \n        sum((sample - mean(sample))^2) / (length(sample) - 1)\n        }\n        \n        paste(\n           \"Mean of 1000 corrected sample variances: \", \n           round(mean(sample.var.cor), 2)\n        )\n        ```\n        \n        ::: {.cell-output .cell-output-stdout}\n        \n        ```\n        [1] \"Mean of 1000 corrected sample variances:  98.32\"\n        ```\n        \n        \n        :::\n        :::\n\n\n\n\n-   The division by $(n-1)$ in the sample variance formula can also be viewed as a way to adjust for the loss of one **degree of freedom** when estimating the population variance from the sample variance:\n\n    -   Degrees of freedom $(df)$ is a mathematical concept that refers to the number of independent pieces of information that are free to vary in the presence of a set of constraints.\n\n    -   This concept will be revisited in upcoming chapters.\n\n    -   Let's consider a simple example to understand this concept:\n\n        -   Assume that we have $5$ boxes that each needs to be filled with a real number such that the sum of the numbers is $10$ (this is referred to as a **constraint**) and we ask $5$ persons to choose the numbers they like as illustrated in the following image: ![](/images/df.jpg)\n\n        -   The first person <i class=\"bi bi-person-fill\" style=\"color: #0171c3; font-size: 22px;\"></i> is free to choose any real number; he chooses $5$ to fill the first box without being constrained by the sum of the numbers.\n\n        -   The second person <i class=\"bi bi-person-fill\" style=\"color: red; font-size: 22px;\"></i> is also free to choose any real number; he chooses $2$ to fill the second box without being constrained by the sum of the numbers.\n\n        -   The third person <i class=\"bi bi-person-fill\" style=\"color: darkgreen; font-size: 22px;\"></i> is also free to choose any real number; he chooses $-3$ to fill the third box without being constrained by the sum of the numbers.\n\n        -   The fourth person <i class=\"bi bi-person-fill\" style=\"color: orange; font-size: 22px;\"></i> is also free to choose any real number; he chooses $7$ to fill the fourth box without being constrained by the sum of the numbers.\n\n        -   On the other contrary, the fifth person <i class=\"bi bi-person-fill\" style=\"color: #89caf2; font-size: 22px;\"></i> is not free to choose any real number; he must choose the number $-1$ to fill the fifth box in order to satisfy the constraint that the sum of the numbers is $10$.\n\n        -   Therefore, there are only $4$ independent pieces of information that are free to vary (i.e., the first four persons).\n\n        -   The fifth person is not free to choose any real number rather his choice is dependent on the choices of the first four persons due to the constraint imposed on the sum of the numbers.\n\n        -   Therefore, we say that we lose one degree of freedom (i.e., $df = n - 1$).\n\n        -   This concept is also applicable to the estimation of the sample variance; which involves the estimation of $\\mu$ from $\\bar{x}$. Using $\\bar{x}$ in the calculation of the sample variance implies that the pieces of informatios $(x_1 - \\bar{x})^2, (x_2 - \\bar{x})^2, \\ldots, (x_n - \\bar{x})^2$, which are used for the computation of the variance, are not independent because they are constrained by the $\\bar{x}$. Simply, there are only $n-1$ independent observations that are free to vary, while the last one can be determined by from the first $n-1$ observations and $\\bar{x}$).\n\n::: {.callout-note style=\"color: #0466c8;\"}\n-   Generally, the degrees of freedom for an estimate is equal to the number of observations minus the number of population parameters estimated while calculating the estimate in question.\n\n-   Applying this rule to the sample variance, we estimate the population mean $\\mu$ from the sample mean $\\bar{x}$, which is one parameter estimated from the sample while computing the sample variance. Therefore, the degrees of freedom for the sample variance is $n - 1$.\n:::\n\n\n## References\n\n-   Lane, D (2023). Introductory Statistics. Retrieved September 14, 2024, from https://stats.libretexts.org/Bookshelves/Introductory_Statistics/Introductory_Statistics\\_(Lane)\n\n<br>\n\n## Add your comments\n",
    "supporting": [
      "B07_sample_variance_stats_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}